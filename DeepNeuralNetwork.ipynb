{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### \n",
    "<img src=\"images/DNN_flow.png\" style=\"width:800px;height:500px;\">\n",
    "<caption><center> Figure 1 : Flow for Constructing a Deep Neural Network</center></caption><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Create and visualize dataset\n",
    "m = 1000     # m = total number of samples (set to even number)\n",
    "n = 2      # n = number of features of input. 2 implies 2 dimensional (2D)\n",
    "\n",
    "# Creation of dataset for Y = 0\n",
    "angles = np.arange(0, 360, 360 / (m//2))   # Half the sample will be Y=0\n",
    "amp_X0 = np.random.randn(m//2)\n",
    "X0 = np.zeros((n+1, m//2))\n",
    "X0[0] = np.sin(angles * np.pi/180) * amp_X0\n",
    "X0[1] = np.cos(angles * np.pi/180) * amp_X0\n",
    "X0[2] = np.zeros((m//2))\n",
    "# print('X0 shape = {}, X0 = \\n{}\\n'.format(X0.shape, X0))\n",
    "\n",
    "# Creation of dataset for Y = 1\n",
    "angles = np.arange(0, 360, 360 / (m - m//2))\n",
    "amp_X1 = np.random.randn((m - m//2))*3\n",
    "X1 = np.zeros((n+1, (m - m//2)))\n",
    "X1[0] = np.sin(angles * np.pi/180) * amp_X1 + amp_X0*3 + 10\n",
    "X1[1] = np.cos(angles * np.pi/180) * amp_X1 + amp_X0*3 + 10\n",
    "X1[2] = np.ones(((m - m//2)))\n",
    "# print('X1 shape = {}, X1 = \\n{}\\n'.format(X1.shape, X1))\n",
    "\n",
    "# Combine and shuffle arrays\n",
    "X = np.concatenate((X0, X1),axis=1)\n",
    "X = X.T\n",
    "np.random.shuffle(X)    # Shuffle only works on axis=0, thus need the X.T before and after\n",
    "X = X.T\n",
    "Y = X[2:]\n",
    "X = X[0:2]\n",
    "\n",
    "# Visualize dataset with plot\n",
    "plt.scatter(X0[0],X0[1])\n",
    "plt.scatter(X1[0],X1[1])\n",
    "plt.xlabel('Feature 0')\n",
    "plt.ylabel('Feature 1')\n",
    "plt.legend(['Label 0', 'Label 1'])\n",
    "\n",
    "# Remove 20% of the dataset to be reserved for testing\n",
    "X_test = X[:,int(np.floor(0.8*m)):]\n",
    "Y_test = Y[:,int(np.floor(0.8*m)):]\n",
    "X = X[:,:int(np.floor(0.8*m))]\n",
    "Y = Y[:,:int(np.floor(0.8*m))]\n",
    "m = Y.shape[1]\n",
    "print('Value of m = {}, Shape X_test = {}, Shape X = {}\\n'.format(m, X_test.shape, X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initialize the Parameters\n",
    "<img src=\"images/initialization.png\" style=\"width:800px;height:500px;\">\n",
    "<caption><center> Figure 2 : Initialize the Parameters</center></caption><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters definition\n",
    "L = 4                 # Total number of layers defined to be 4\n",
    "# nl = [5, 4, 3, 1]    # Define number of nodes / features for each layer\n",
    "nl = [56, 128, 56, 1]\n",
    "\n",
    "# Include input features into layers of DNN\n",
    "n_l = [n] \n",
    "n_l.extend(nl)\n",
    "\n",
    "# Numpy initialization of weights and bias\n",
    "W_numpy = [0]\n",
    "b_numpy = [0]\n",
    "for i in range(1, len(n_l)):\n",
    "    W_numpy_temp = np.random.randn(n_l[i], n_l[i-1]) * 0.01    # Multiply by a small number 0.01 to reduce the value of z, to speed up learning (esp for sigmoid activation)\n",
    "    W_numpy.append(W_numpy_temp)\n",
    "    \n",
    "    b_numpy_temp = np.ones((n_l[i], 1)) * 0.001   # Add a small bias to prevent divide by 0 operation\n",
    "    b_numpy.append(b_numpy_temp)\n",
    "\n",
    "print('Shapes of W_numpy are: {}, {}, {}, {}'.format(W_numpy[1].shape, W_numpy[2].shape, W_numpy[3].shape, W_numpy[4].shape))\n",
    "print('Shapes of b_numpy are: {}, {}, {}, {}\\n'.format(b_numpy[1].shape, b_numpy[2].shape, b_numpy[3].shape, b_numpy[4].shape))\n",
    "\n",
    "\n",
    "## Pytorch initialization of weights and bias\n",
    "W_torch = [0]\n",
    "b_torch = [0]\n",
    "for i in range(1, len(W_numpy)):    # Convert the numpy array to torch\n",
    "    W_torch.append(torch.tensor(W_numpy[i]))\n",
    "    b_torch.append(torch.tensor(b_numpy[i]))\n",
    "                   \n",
    "print('Shapes of W_torch are: {}, {}, {}, {}'.format(W_torch[1].shape, W_torch[2].shape, W_torch[3].shape, W_torch[4].shape))\n",
    "print('Shapes of b_torch are: {}, {}, {}, {}\\n'.format(b_torch[1].shape, b_torch[2].shape, b_torch[3].shape, b_torch[4].shape))\n",
    "\n",
    "\n",
    "## Tensorflow initialization of weights and bias\n",
    "W_tf = [0]\n",
    "b_tf = [0]\n",
    "for i in range(1, len(W_numpy)):    # Convert the numpy array to torch\n",
    "    W_tf.append(tf.convert_to_tensor(W_numpy[i]))\n",
    "    b_tf.append(tf.convert_to_tensor(b_numpy[i]))\n",
    "                   \n",
    "print('Shapes of W_tf are: {}, {}, {}, {}'.format(W_tf[1].shape, W_tf[2].shape, W_tf[3].shape, W_tf[4].shape))\n",
    "print('Shapes of b_tf are: {}, {}, {}, {}\\n'.format(b_tf[1].shape, b_tf[2].shape, b_tf[3].shape, b_tf[4].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Perform Forward Pass\n",
    "<img src=\"images/forward_pass.png\" style=\"width:800px;height:500px;\">\n",
    "<caption><center> Figure 3 : Perform forward pass </center></caption><br>\n",
    "\n",
    "Note:\n",
    "- For all hidden layers, ReLU activation function is used\n",
    "- For last layer (i.e. output layer), Sigmoid activation function is used\n",
    "- Cross Entropy Loss is used as the Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy forward pass\n",
    "# Definition of sigmoid and relu function for numpy\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(Z,0)\n",
    "\n",
    "Z_numpy = [0]\n",
    "A_numpy = [X]\n",
    "Y_numpy = Y\n",
    "for i in range(1, len(n_l)):\n",
    "#     print(W_numpy[i].shape, A_numpy[i-1].shape, np.matmul(W_numpy[i],A_numpy[i-1]).shape)\n",
    "    Z_temp = np.matmul(W_numpy[i],A_numpy[i-1]) + b_numpy[i]\n",
    "    Z_numpy.append(Z_temp)\n",
    "    if i != len(n_l):\n",
    "        A_temp = relu(Z_temp)   # ReLU activation for all hidden layers\n",
    "        A_numpy.append(A_temp) \n",
    "    else:\n",
    "        A_temp = sigmoid(Z_temp)  # Sigmoid activation for last (output) layer. Binary classification\n",
    "        A_numpy.append(A_temp)   \n",
    "J_numpy = (-1)*np.sum(Y_numpy*np.log(A_numpy[L]) + (1-Y_numpy)*np.log(1-A_numpy[L]))/m\t\n",
    "\n",
    "# Pytorch forward pass\n",
    "Z_torch = [0]\n",
    "A_torch = [torch.tensor(X)]\n",
    "Y_torch = torch.tensor(Y)\n",
    "sigmoid = torch.nn.Sigmoid()    # Use sigmoid definition from pytorch nn library\n",
    "relu = torch.nn.ReLU()          # Use ReLU definition from pytorch nn library\n",
    "for i in range(1, len(n_l)):\n",
    "#     print(W_torch[i].shape, A_torch[i-1].shape, torch.matmul(W_torch[i],A_torch[i-1]).shape)\n",
    "    Z_temp = torch.matmul(W_torch[i],A_torch[i-1]) + b_torch[i]\n",
    "    Z_torch.append(Z_temp)\n",
    "    if i != len(n_l):\n",
    "        A_temp = relu(Z_temp)   # ReLU activation for all hidden layers\n",
    "        A_torch.append(A_temp) \n",
    "    else:\n",
    "        A_temp = sigmoid(Z_temp)  # Sigmoid activation for last (output) layer. Binary classification\n",
    "        A_torch.append(A_temp)   \n",
    "J_torch = (-1)*torch.sum(Y_torch*torch.log(A_torch[L]) +  (1-Y_torch)*torch.log(1-A_torch[L]))/m\n",
    "        \n",
    "        \n",
    "# Tensorflow forward pass\n",
    "Z_tf = [0]\n",
    "A_tf = [tf.convert_to_tensor(X)]\n",
    "Y_tf = tf.convert_to_tensor(Y)\n",
    "for i in range(1, len(n_l)):\n",
    "#     print(W_tf[i].shape, A_tf[i-1].shape, tf.matmul(W_tf[i],A_tf[i-1]).shape)\n",
    "\n",
    "    Z_temp = tf.matmul(W_tf[i],A_tf[i-1]) + b_tf[i]\n",
    "    Z_tf.append(Z_temp)\n",
    "    if i != len(n_l):\n",
    "        A_temp = tf.keras.activations.relu(Z_temp)   # ReLU activation for all hidden layers\n",
    "        A_tf.append(A_temp) \n",
    "    else:\n",
    "        A_temp = tf.keras.activations.sigmoid(Z_temp)  # Sigmoid activation for last (output) layer. Binary classification\n",
    "        A_tf.append(A_temp)  \n",
    "J_tf = (-1)*tf.math.reduce_sum(Y_tf*tf.math.log(A_tf[L]) +  (1-Y_tf)*tf.math.log(1-A_tf[L]))/m\n",
    "\n",
    "print('\\nLoss function calculated from numpy, pytorch and tensorflow are {}, {} and {}'.format(J_numpy, J_torch, J_tf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Perform Backward Pass\n",
    "<img src=\"images/backward_pass.png\" style=\"width:800px;height:500px;\">\n",
    "<caption><center> Figure 4 : Perform backward pass </center></caption><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy backward pass\n",
    "dA_numpy = []\n",
    "dZ_numpy = []\n",
    "dW_numpy = []\n",
    "db_numpy = []\n",
    "\n",
    "# For last (output) layer\n",
    "dAL = - (np.divide(Y_numpy, A_numpy[L]) - np.divide(1 - Y_numpy, 1 - A_numpy[L]))   # Find grad of last activation layer due to CEL cost function\n",
    "dZ_temp = dAL * A_numpy[L] * (1 - A_numpy[L])    # Backward function for sigmoid activation\n",
    "dW_temp = np.matmul(dZ_temp, A_numpy[L-1].T) / m\n",
    "db_temp = np.sum(dZ_temp, axis=1, keepdims=True)/m\n",
    "dA_numpy.append(dAL)\n",
    "dZ_numpy.append(dZ_temp)\n",
    "dW_numpy.append(dW_temp)\n",
    "db_numpy.append(db_temp)\n",
    "\n",
    "# For all other layers\n",
    "for i in reversed(range(1, L)):\n",
    "    dA_temp = np.matmul(W_numpy[i+1].T,dZ_numpy[-1])\t\n",
    "    dZ_temp = np.array(dA_temp, copy=True)         # Backward pass for ReLU, step 1: copy array\n",
    "    dZ_temp[Z_numpy[i] <= 0] = 0                   # Backward pass for ReLU, step 2: Set all dZ=0 where Z < 0\n",
    "    dW_temp = np.matmul(dZ_temp, A_numpy[i-1].T) / m\n",
    "    db_temp = np.sum(dZ_temp, axis=1, keepdims=True)/m\n",
    "    dA_numpy.append(dA_temp)\n",
    "    dZ_numpy.append(dZ_temp)\n",
    "    dW_numpy.append(dW_temp)\n",
    "    db_numpy.append(db_temp)\n",
    "    \t\n",
    "    \n",
    "# Reverse the list so that indexes are respective to layer number.\n",
    "# This is so as for backward pass, all calculations are done from last layer to first layer\n",
    "dA_numpy.append(0)    # Append addition 0 so that after reverse, dA0 correspond to A0 with same dimension\n",
    "dZ_numpy.append(0)\n",
    "dW_numpy.append(0)\n",
    "db_numpy.append(0)\n",
    "dA_numpy = list(reversed(dA_numpy))\n",
    "dZ_numpy = list(reversed(dZ_numpy))\n",
    "dW_numpy = list(reversed(dW_numpy))\n",
    "db_numpy = list(reversed(db_numpy))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Pytorch backward pass\n",
    "dA_torch = []\n",
    "dZ_torch = []\n",
    "dW_torch = []\n",
    "db_torch = []\n",
    "\n",
    "# For last (output) layer\n",
    "dAL = - (torch.div(Y_torch, A_torch[L]) - torch.div(1 - Y_torch, 1 - A_torch[L]))   # Find grad of last activation layer due to CEL cost function\n",
    "dZ_temp = dAL * A_torch[L] * (1 - A_torch[L])    # Backward function for sigmoid activation\n",
    "dW_temp = torch.matmul(dZ_temp, A_torch[L-1].T) / m\n",
    "db_temp = torch.sum(dZ_temp, dim=1, keepdim=True)/m\n",
    "dA_torch.append(dAL)\n",
    "dZ_torch.append(dZ_temp)\n",
    "dW_torch.append(dW_temp)\n",
    "db_torch.append(db_temp)\n",
    "\n",
    "# For all other layers\n",
    "for i in reversed(range(1, L)):\n",
    "    dA_temp = torch.matmul(W_torch[i+1].T,dZ_torch[-1])\t\n",
    "    dZ_temp = dA_temp.clone()                      # Backward pass for ReLU, step 1: copy array\n",
    "    dZ_temp[Z_torch[i] <= 0] = 0                   # Backward pass for ReLU, step 2: Set all dZ=0 where Z < 0\n",
    "    dW_temp = torch.matmul(dZ_temp, A_torch[i-1].T) / m\n",
    "    db_temp = torch.sum(dZ_temp, dim=1, keepdim=True)/m\n",
    "    dA_torch.append(dA_temp)\n",
    "    dZ_torch.append(dZ_temp)\n",
    "    dW_torch.append(dW_temp)\n",
    "    db_torch.append(db_temp)\n",
    "    \n",
    "# Reverse the list so that indexes are respective to layer number.\n",
    "# This is so as for backward pass, all calculations are done from last layer to first layer\n",
    "dA_torch.append(0)    # Append addition 0 so that after reverse, dA0 correspond to A0 with same dimension\n",
    "dZ_torch.append(0)\n",
    "dW_torch.append(0)\n",
    "db_torch.append(0)\n",
    "dA_torch = list(reversed(dA_torch))\n",
    "dZ_torch = list(reversed(dZ_torch))\n",
    "dW_torch = list(reversed(dW_torch))\n",
    "db_torch = list(reversed(db_torch))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Tensorflow backward pass\n",
    "dA_tf = []\n",
    "dZ_tf = []\n",
    "dW_tf = []\n",
    "db_tf = []\n",
    "\n",
    "# For last (output) layer\n",
    "dAL = - (tf.math.divide(Y_tf, A_tf[L]) - tf.math.divide(1 - Y_tf, 1 - A_tf[L]))   # Find grad of last activation layer due to CEL cost function\n",
    "dZ_temp = dAL * A_tf[L] * (1 - A_tf[L])    # Backward function for sigmoid activation\n",
    "dW_temp = tf.matmul(dZ_temp, tf.transpose(A_tf[L-1])) / m\n",
    "db_temp = tf.math.reduce_sum(dZ_temp, axis=1, keepdims=True)/m\n",
    "dA_tf.append(dAL)\n",
    "dZ_tf.append(dZ_temp)\n",
    "dW_tf.append(dW_temp)\n",
    "db_tf.append(db_temp)\n",
    "\n",
    "# For all other layers\n",
    "for i in reversed(range(1, L)):\n",
    "    dA_temp = tf.matmul(tf.transpose(W_tf[i+1]),dZ_tf[-1])\t\n",
    "    dZ_temp = tf.identity(dA_temp)                     # Backward pass for ReLU, step 1: copy array\n",
    "    dZ_temp = tf.where(Z_tf[i] > 0. , dZ_temp , [0])\n",
    "    dW_temp = tf.matmul(dZ_temp, tf.transpose(A_tf[i-1])) / m\n",
    "    db_temp = tf.math.reduce_sum(dZ_temp, axis=1, keepdims=True)/m\n",
    "    dA_tf.append(dA_temp)\n",
    "    dZ_tf.append(dZ_temp)\n",
    "    dW_tf.append(dW_temp)\n",
    "    db_tf.append(db_temp)\n",
    "    \n",
    "# Reverse the list so that indexes are respective to layer number.\n",
    "# This is so as for backward pass, all calculations are done from last layer to first layer\n",
    "dA_tf.append(0)    # Append addition 0 so that after reverse, dA0 correspond to A0 with same dimension\n",
    "dZ_tf.append(0)\n",
    "dW_tf.append(0)\n",
    "db_tf.append(0)\n",
    "dA_tf = list(reversed(dA_tf))\n",
    "dZ_tf = list(reversed(dZ_tf))\n",
    "dW_tf = list(reversed(dW_tf))\n",
    "db_tf = list(reversed(db_tf))\n",
    "\n",
    "print(dW_tf[2], '\\n', dW_torch[2], '\\n', dW_numpy[2])\n",
    "print(A_tf[0], '\\n', 'A_torch[0]', '\\n', A_numpy[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Update Parameters\n",
    "<img src=\"images/update_params.png\" style=\"width:800px;height:500px;\">\n",
    "<caption><center> Figure 5 : Update Parameters </center></caption><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update parameters\n",
    "learn_rate = 0.01\n",
    "\n",
    "# Numpy update\n",
    "for i in range(1,L+1):\n",
    "    W_numpy[i] = W_numpy[i] - learn_rate*dW_numpy[i]\n",
    "    b_numpy[i] = b_numpy[i] - learn_rate*db_numpy[i]\n",
    "\n",
    "# Pytorch update\n",
    "for i in range(1,L+1):\n",
    "    W_torch[i] = W_torch[i] - learn_rate*dW_torch[i]\n",
    "    b_torch[i] = b_torch[i] - learn_rate*db_torch[i]\n",
    "    \n",
    "# Tensorflow update\n",
    "for i in range(1,L+1):\n",
    "    W_tf[i] = W_tf[i] - learn_rate*dW_tf[i]\n",
    "    b_tf[i] = b_tf[i] - learn_rate*db_tf[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Perform prediction on model and calculate accuracy\n",
    "This step replicates exactly the same steps as Step 2 with the following changes:\n",
    "1. A_numpy = [X_test]\n",
    "2. Y_numpy = [Y_test]\n",
    "3. A_torch = [torch.tensor(X_test)]\n",
    "4. Y_torch = torch.tensor(Y_test)\n",
    "5. A_tf = [tf.convert_to_tensor(X_test)]\n",
    "6. Y_tf = tf.convert_to_tensor(Y_test)\n",
    "7. See one cell later on prediction of output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy forward pass\n",
    "# Definition of sigmoid and relu function for numpy\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(Z,0)\n",
    "\n",
    "Z_numpy = [0]\n",
    "A_numpy = [X_test]\n",
    "Y_numpy = Y_test\n",
    "for i in range(1, len(n_l)):\n",
    "#     print(W_numpy[i].shape, A_numpy[i-1].shape, np.matmul(W_numpy[i],A_numpy[i-1]).shape)\n",
    "    Z_temp = np.matmul(W_numpy[i],A_numpy[i-1]) + b_numpy[i]\n",
    "    Z_numpy.append(Z_temp)\n",
    "    if i != len(n_l):\n",
    "        A_temp = relu(Z_temp)   # ReLU activation for all hidden layers\n",
    "        A_numpy.append(A_temp) \n",
    "    else:\n",
    "        A_temp = sigmoid(Z_temp)  # Sigmoid activation for last (output) layer. Binary classification\n",
    "        A_numpy.append(A_temp)   \n",
    "J_numpy = (-1)*np.sum(Y_numpy*np.log(A_numpy[L]) + (1-Y_numpy)*np.log(1-A_numpy[L]))/m\t\n",
    "\n",
    "# Pytorch forward pass\n",
    "Z_torch = [0]\n",
    "A_torch = [torch.tensor(X_test)]\n",
    "Y_torch = torch.tensor(Y_test)\n",
    "sigmoid = torch.nn.Sigmoid()    # Use sigmoid definition from pytorch nn library\n",
    "relu = torch.nn.ReLU()          # Use ReLU definition from pytorch nn library\n",
    "for i in range(1, len(n_l)):\n",
    "#     print(W_torch[i].shape, A_torch[i-1].shape, torch.matmul(W_torch[i],A_torch[i-1]).shape)\n",
    "    Z_temp = torch.matmul(W_torch[i],A_torch[i-1]) + b_torch[i]\n",
    "    Z_torch.append(Z_temp)\n",
    "    if i != len(n_l):\n",
    "        A_temp = relu(Z_temp)   # ReLU activation for all hidden layers\n",
    "        A_torch.append(A_temp) \n",
    "    else:\n",
    "        A_temp = sigmoid(Z_temp)  # Sigmoid activation for last (output) layer. Binary classification\n",
    "        A_torch.append(A_temp)   \n",
    "J_torch = (-1)*torch.sum(Y_torch*torch.log(A_torch[L]) +  (1-Y_torch)*torch.log(1-A_torch[L]))/m\n",
    "        \n",
    "        \n",
    "# Tensorflow forward pass\n",
    "Z_tf = [0]\n",
    "A_tf = [tf.convert_to_tensor(X_test)]\n",
    "Y_tf = tf.convert_to_tensor(Y_test)\n",
    "for i in range(1, len(n_l)):\n",
    "#     print(W_tf[i].shape, A_tf[i-1].shape, tf.matmul(W_tf[i],A_tf[i-1]).shape)\n",
    "\n",
    "    Z_temp = tf.matmul(W_tf[i],A_tf[i-1]) + b_tf[i]\n",
    "    Z_tf.append(Z_temp)\n",
    "    if i != len(n_l):\n",
    "        A_temp = tf.keras.activations.relu(Z_temp)   # ReLU activation for all hidden layers\n",
    "        A_tf.append(A_temp) \n",
    "    else:\n",
    "        A_temp = tf.keras.activations.sigmoid(Z_temp)  # Sigmoid activation for last (output) layer. Binary classification\n",
    "        A_tf.append(A_temp)  \n",
    "J_tf = (-1)*tf.math.reduce_sum(Y_tf*tf.math.log(A_tf[L]) +  (1-Y_tf)*tf.math.log(1-A_tf[L]))/m\n",
    "\n",
    "print('\\nLoss function calculated from numpy, pytorch and tensorflow are {}, {} and {}'.format(J_numpy, J_torch, J_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict output using the output of the last activation\n",
    "\n",
    "# Numpy prediction\n",
    "predict_numpy = A_numpy[L].round()\n",
    "accuracy_numpy = np.count_nonzero(predict_numpy == Y_numpy) / Y_numpy.shape[1] * 100\n",
    "\n",
    "# Pytorch prediction\n",
    "predict_torch = A_torch[L].round()\n",
    "accuracy_torch = torch.sum(predict_torch == Y_torch) / float(Y_torch.shape[1]) * 100\n",
    "\n",
    "# Tensorflow prediction\n",
    "predict_tf = tf.math.round(A_tf[L])\n",
    "accuracy_tf = tf.reduce_sum(tf.cast(predict_tf == Y_tf, tf.float32)) / Y_tf.shape[1] * 100\n",
    "\n",
    "print('Accuracy of numpy, pytorch and tensorflow prediction are {}%, {}% and {}%respectively\\n'.format(accuracy_numpy, accuracy_torch, accuracy_tf))\n",
    "\n",
    "# Plot test data and prediction\n",
    "# Ground Truth\n",
    "X0_test = X_test[:,Y_test[0,:] == 0]\n",
    "X1_test = X_test[:,Y_test[0,:] == 1]\n",
    "\n",
    "X0_numpy = X_test[:,predict_numpy[0,:] == 0]\n",
    "X1_numpy = X_test[:,predict_numpy[0,:] == 1]\n",
    "\n",
    "X0_torch = X_test[:,predict_torch[0,:] == 0]\n",
    "X1_torch = X_test[:,predict_torch[0,:] == 1]\n",
    "\n",
    "X0_tf = X_test[:,predict_tf[0,:] == 0]\n",
    "X1_tf = X_test[:,predict_tf[0,:] == 1]\n",
    "\n",
    "\n",
    "# Ground Truth\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.scatter(X0_test[0], X0_test[1])\n",
    "plt.scatter(X1_test[0], X1_test[1])\n",
    "plt.legend(['0', '1'])\n",
    "plt.title('Ground Truth')\n",
    "\n",
    "# Numpy Prediction\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(X0_numpy[0], X0_numpy[1])\n",
    "plt.scatter(X1_numpy[0], X1_numpy[1])\n",
    "plt.legend(['0', '1'])\n",
    "plt.title('Numpy Prediction')\n",
    "\n",
    "# Pytorch Prediction\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(X0_torch[0], X0_torch[1])\n",
    "plt.scatter(X1_torch[0], X1_torch[1])\n",
    "plt.legend(['0', '1'])\n",
    "plt.title('Pytorch Prediction')\n",
    "\n",
    "# Tensorflow Prediction\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(X0_tf[0], X0_tf[1])\n",
    "plt.scatter(X1_tf[0], X1_tf[1])\n",
    "plt.legend(['0', '1'])\n",
    "plt.title('Tensorflow Prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Repeat steps 2 to 5 with a certain number of epoch to learn the relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Create and visualize dataset\n",
    "m = 1000     # m = total number of samples (set to even number)\n",
    "n = 2      # n = number of features of input. 2 implies 2 dimensional (2D)\n",
    "\n",
    "# Creation of dataset for Y = 0\n",
    "angles = np.arange(0, 360, 360 / (m//2))   # Half the sample will be Y=0\n",
    "amp_X0 = np.random.randn(m//2)\n",
    "X0 = np.zeros((n+1, m//2))\n",
    "X0[0] = np.sin(angles * np.pi/180) * amp_X0\n",
    "X0[1] = np.cos(angles * np.pi/180) * amp_X0\n",
    "X0[2] = np.zeros((m//2))\n",
    "# print('X0 shape = {}, X0 = \\n{}\\n'.format(X0.shape, X0))\n",
    "\n",
    "# Creation of dataset for Y = 1\n",
    "angles = np.arange(0, 360, 360 / (m - m//2))\n",
    "amp_X1 = np.random.randn((m - m//2))*3\n",
    "X1 = np.zeros((n+1, (m - m//2)))\n",
    "X1[0] = np.sin(angles * np.pi/180) * amp_X1 + amp_X0*3\n",
    "X1[1] = np.cos(angles * np.pi/180) * amp_X1 + amp_X0*3\n",
    "X1[2] = np.ones(((m - m//2)))\n",
    "# print('X1 shape = {}, X1 = \\n{}\\n'.format(X1.shape, X1))\n",
    "\n",
    "# Combine and shuffle arrays\n",
    "X = np.concatenate((X0, X1),axis=1)\n",
    "X = X.T\n",
    "np.random.shuffle(X)    # Shuffle only works on axis=0, thus need the X.T before and after\n",
    "X = X.T\n",
    "Y = X[2:]\n",
    "X = X[0:2]\n",
    "\n",
    "# Visualize dataset with plot\n",
    "plt.scatter(X0[0],X0[1])\n",
    "plt.scatter(X1[0],X1[1])\n",
    "plt.xlabel('Feature 0')\n",
    "plt.ylabel('Feature 1')\n",
    "plt.legend(['Label 0', 'Label 1'])\n",
    "\n",
    "# Remove 20% of the dataset to be reserved for testing\n",
    "X_test = X[:,int(np.floor(0.8*m)):]\n",
    "Y_test = Y[:,int(np.floor(0.8*m)):]\n",
    "X = X[:,:int(np.floor(0.8*m))]\n",
    "Y = Y[:,:int(np.floor(0.8*m))]\n",
    "m = Y.shape[1]\n",
    "print('Value of m = {}, Shape X_test = {}, Shape X = {}\\n'.format(m, X_test.shape, X.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters definition\n",
    "L = 4                 # Total number of layers defined to be 4\n",
    "nl = [5, 4, 3, 1]    # Define number of nodes / features for each layer\n",
    "\n",
    "# Include input features into layers of DNN\n",
    "n_l = [n] \n",
    "n_l.extend(nl)\n",
    "\n",
    "# Numpy initialization of weights and bias\n",
    "W_numpy = [0]\n",
    "b_numpy = [0]\n",
    "for i in range(1, len(n_l)):\n",
    "    W_numpy_temp = np.random.randn(n_l[i], n_l[i-1]) * 0.01    # Multiply by a small number 0.01 to reduce the value of z, to speed up learning (esp for sigmoid activation)\n",
    "    W_numpy.append(W_numpy_temp)\n",
    "    \n",
    "    b_numpy_temp = np.ones((n_l[i], 1)) * 0.001   # Add a small bias to prevent divide by 0 operation\n",
    "    b_numpy.append(b_numpy_temp)\n",
    "\n",
    "print('Shapes of W_numpy are: {}, {}, {}, {}'.format(W_numpy[1].shape, W_numpy[2].shape, W_numpy[3].shape, W_numpy[4].shape))\n",
    "print('Shapes of b_numpy are: {}, {}, {}, {}\\n'.format(b_numpy[1].shape, b_numpy[2].shape, b_numpy[3].shape, b_numpy[4].shape))\n",
    "\n",
    "\n",
    "## Pytorch initialization of weights and bias\n",
    "W_torch = [0]\n",
    "b_torch = [0]\n",
    "for i in range(1, len(W_numpy)):    # Convert the numpy array to torch\n",
    "    W_torch.append(torch.tensor(W_numpy[i]))\n",
    "    b_torch.append(torch.tensor(b_numpy[i]))\n",
    "                   \n",
    "print('Shapes of W_torch are: {}, {}, {}, {}'.format(W_torch[1].shape, W_torch[2].shape, W_torch[3].shape, W_torch[4].shape))\n",
    "print('Shapes of b_torch are: {}, {}, {}, {}\\n'.format(b_torch[1].shape, b_torch[2].shape, b_torch[3].shape, b_torch[4].shape))\n",
    "\n",
    "\n",
    "## Tensorflow initialization of weights and bias\n",
    "W_tf = [0]\n",
    "b_tf = [0]\n",
    "for i in range(1, len(W_numpy)):    # Convert the numpy array to torch\n",
    "    W_tf.append(tf.convert_to_tensor(W_numpy[i]))\n",
    "    b_tf.append(tf.convert_to_tensor(b_numpy[i]))\n",
    "                   \n",
    "print('Shapes of W_tf are: {}, {}, {}, {}'.format(W_tf[1].shape, W_tf[2].shape, W_tf[3].shape, W_tf[4].shape))\n",
    "print('Shapes of b_tf are: {}, {}, {}, {}\\n'.format(b_tf[1].shape, b_tf[2].shape, b_tf[3].shape, b_tf[4].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X, W_numpy, b_numpy, W_torch, b_torch, W_tf, b_tf, Y, n_l, L, m):\n",
    "    # Numpy forward pass\n",
    "    # Definition of sigmoid and relu function for numpy\n",
    "    def sigmoid(Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "    def relu(Z):\n",
    "        return np.maximum(Z,0)\n",
    "\n",
    "    Z_numpy = [0]\n",
    "    A_numpy = [X]\n",
    "    Y_numpy = Y\n",
    "    for i in range(1, len(n_l)):\n",
    "    #     print(W_numpy[i].shape, A_numpy[i-1].shape, np.matmul(W_numpy[i],A_numpy[i-1]).shape)\n",
    "        Z_temp = np.matmul(W_numpy[i],A_numpy[i-1]) + b_numpy[i]\n",
    "        Z_numpy.append(Z_temp)\n",
    "        if i != len(n_l):\n",
    "            A_temp = relu(Z_temp)   # ReLU activation for all hidden layers\n",
    "            A_numpy.append(A_temp) \n",
    "        else:\n",
    "            A_temp = sigmoid(Z_temp)  # Sigmoid activation for last (output) layer. Binary classification\n",
    "            A_numpy.append(A_temp)   \n",
    "    J_numpy = (-1)*np.sum(Y_numpy*np.log(A_numpy[L]) + (1-Y_numpy)*np.log(1-A_numpy[L]))/m\t\n",
    "\n",
    "    # Pytorch forward pass\n",
    "    Z_torch = [0]\n",
    "    A_torch = [torch.tensor(X)]\n",
    "    Y_torch = torch.tensor(Y)\n",
    "    sigmoid = torch.nn.Sigmoid()    # Use sigmoid definition from pytorch nn library\n",
    "    relu = torch.nn.ReLU()          # Use ReLU definition from pytorch nn library\n",
    "    for i in range(1, len(n_l)):\n",
    "    #     print(W_torch[i].shape, A_torch[i-1].shape, torch.matmul(W_torch[i],A_torch[i-1]).shape)\n",
    "        Z_temp = torch.matmul(W_torch[i],A_torch[i-1]) + b_torch[i]\n",
    "        Z_torch.append(Z_temp)\n",
    "        if i != len(n_l):\n",
    "            A_temp = relu(Z_temp)   # ReLU activation for all hidden layers\n",
    "            A_torch.append(A_temp) \n",
    "        else:\n",
    "            A_temp = sigmoid(Z_temp)  # Sigmoid activation for last (output) layer. Binary classification\n",
    "            A_torch.append(A_temp)   \n",
    "    J_torch = (-1)*torch.sum(Y_torch*torch.log(A_torch[L]) +  (1-Y_torch)*torch.log(1-A_torch[L]))/m\n",
    "\n",
    "\n",
    "    # Tensorflow forward pass\n",
    "    Z_tf = [0]\n",
    "    A_tf = [tf.convert_to_tensor(X)]\n",
    "    Y_tf = tf.convert_to_tensor(Y)\n",
    "    for i in range(1, len(n_l)):\n",
    "    #     print(W_tf[i].shape, A_tf[i-1].shape, tf.matmul(W_tf[i],A_tf[i-1]).shape)\n",
    "\n",
    "        Z_temp = tf.matmul(W_tf[i],A_tf[i-1]) + b_tf[i]\n",
    "        Z_tf.append(Z_temp)\n",
    "        if i != len(n_l):\n",
    "            A_temp = tf.keras.activations.relu(Z_temp)   # ReLU activation for all hidden layers\n",
    "            A_tf.append(A_temp) \n",
    "        else:\n",
    "            A_temp = tf.keras.activations.sigmoid(Z_temp)  # Sigmoid activation for last (output) layer. Binary classification\n",
    "            A_tf.append(A_temp)  \n",
    "    J_tf = (-1)*tf.math.reduce_sum(Y_tf*tf.math.log(A_tf[L]) +  (1-Y_tf)*tf.math.log(1-A_tf[L]))/m\n",
    "\n",
    "    print('\\nLoss function calculated from numpy, pytorch and tensorflow are {}, {} and {}'.format(J_numpy, J_torch, J_tf))\n",
    "    \n",
    "    return Z_numpy, A_numpy, Z_torch, A_torch, Z_tf, A_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(Z_numpy, A_numpy, Z_torch, A_torch, Z_tf, A_tf, Y_numpy, Y_torch, Y_tf, L, m):\n",
    "    # Numpy backward pass\n",
    "    dA_numpy = []\n",
    "    dZ_numpy = []\n",
    "    dW_numpy = []\n",
    "    db_numpy = []\n",
    "\n",
    "    # For last (output) layer\n",
    "    dAL = - (np.divide(Y_numpy, A_numpy[L]) - np.divide(1 - Y_numpy, 1 - A_numpy[L]))   # Find grad of last activation layer due to CEL cost function\n",
    "    dZ_temp = dAL * A_numpy[L] * (1 - A_numpy[L])    # Backward function for sigmoid activation\n",
    "    dW_temp = np.matmul(dZ_temp, A_numpy[L-1].T) / m\n",
    "    db_temp = np.sum(dZ_temp, axis=1, keepdims=True)/m\n",
    "    dA_numpy.append(dAL)\n",
    "    dZ_numpy.append(dZ_temp)\n",
    "    dW_numpy.append(dW_temp)\n",
    "    db_numpy.append(db_temp)\n",
    "\n",
    "    # For all other layers\n",
    "    for i in reversed(range(1, L)):\n",
    "        dA_temp = np.matmul(W_numpy[i+1].T,dZ_numpy[-1])\t\n",
    "        dZ_temp = np.array(dA_temp, copy=True)         # Backward pass for ReLU, step 1: copy array\n",
    "        dZ_temp[Z_numpy[i] <= 0] = 0                   # Backward pass for ReLU, step 2: Set all dZ=0 where Z < 0\n",
    "        dW_temp = np.matmul(dZ_temp, A_numpy[i-1].T) / m\n",
    "        db_temp = np.sum(dZ_temp, axis=1, keepdims=True)/m\n",
    "        dA_numpy.append(dA_temp)\n",
    "        dZ_numpy.append(dZ_temp)\n",
    "        dW_numpy.append(dW_temp)\n",
    "        db_numpy.append(db_temp)\n",
    "\n",
    "\n",
    "    # Reverse the list so that indexes are respective to layer number.\n",
    "    # This is so as for backward pass, all calculations are done from last layer to first layer\n",
    "    dA_numpy.append(0)    # Append addition 0 so that after reverse, dA0 correspond to A0 with same dimension\n",
    "    dZ_numpy.append(0)\n",
    "    dW_numpy.append(0)\n",
    "    db_numpy.append(0)\n",
    "    dA_numpy = list(reversed(dA_numpy))\n",
    "    dZ_numpy = list(reversed(dZ_numpy))\n",
    "    dW_numpy = list(reversed(dW_numpy))\n",
    "    db_numpy = list(reversed(db_numpy))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Pytorch backward pass\n",
    "    dA_torch = []\n",
    "    dZ_torch = []\n",
    "    dW_torch = []\n",
    "    db_torch = []\n",
    "\n",
    "    # For last (output) layer\n",
    "    dAL = - (torch.div(Y_torch, A_torch[L]) - torch.div(1 - Y_torch, 1 - A_torch[L]))   # Find grad of last activation layer due to CEL cost function\n",
    "    dZ_temp = dAL * A_torch[L] * (1 - A_torch[L])    # Backward function for sigmoid activation\n",
    "    dW_temp = torch.matmul(dZ_temp, A_torch[L-1].T) / m\n",
    "    db_temp = torch.sum(dZ_temp, dim=1, keepdim=True)/m\n",
    "    dA_torch.append(dAL)\n",
    "    dZ_torch.append(dZ_temp)\n",
    "    dW_torch.append(dW_temp)\n",
    "    db_torch.append(db_temp)\n",
    "\n",
    "    # For all other layers\n",
    "    for i in reversed(range(1, L)):\n",
    "        dA_temp = torch.matmul(W_torch[i+1].T,dZ_torch[-1])\t\n",
    "        dZ_temp = dA_temp.clone()                      # Backward pass for ReLU, step 1: copy array\n",
    "        dZ_temp[Z_torch[i] <= 0] = 0                   # Backward pass for ReLU, step 2: Set all dZ=0 where Z < 0\n",
    "        dW_temp = torch.matmul(dZ_temp, A_torch[i-1].T) / m\n",
    "        db_temp = torch.sum(dZ_temp, dim=1, keepdim=True)/m\n",
    "        dA_torch.append(dA_temp)\n",
    "        dZ_torch.append(dZ_temp)\n",
    "        dW_torch.append(dW_temp)\n",
    "        db_torch.append(db_temp)\n",
    "\n",
    "    # Reverse the list so that indexes are respective to layer number.\n",
    "    # This is so as for backward pass, all calculations are done from last layer to first layer\n",
    "    dA_torch.append(0)    # Append addition 0 so that after reverse, dA0 correspond to A0 with same dimension\n",
    "    dZ_torch.append(0)\n",
    "    dW_torch.append(0)\n",
    "    db_torch.append(0)\n",
    "    dA_torch = list(reversed(dA_torch))\n",
    "    dZ_torch = list(reversed(dZ_torch))\n",
    "    dW_torch = list(reversed(dW_torch))\n",
    "    db_torch = list(reversed(db_torch))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Tensorflow backward pass\n",
    "    dA_tf = []\n",
    "    dZ_tf = []\n",
    "    dW_tf = []\n",
    "    db_tf = []\n",
    "\n",
    "    # For last (output) layer\n",
    "    dAL = - (tf.math.divide(Y_tf, A_tf[L]) - tf.math.divide(1 - Y_tf, 1 - A_tf[L]))   # Find grad of last activation layer due to CEL cost function\n",
    "    dZ_temp = dAL * A_tf[L] * (1 - A_tf[L])    # Backward function for sigmoid activation\n",
    "    dW_temp = tf.matmul(dZ_temp, tf.transpose(A_tf[L-1])) / m\n",
    "    db_temp = tf.math.reduce_sum(dZ_temp, axis=1, keepdims=True)/m\n",
    "    dA_tf.append(dAL)\n",
    "    dZ_tf.append(dZ_temp)\n",
    "    dW_tf.append(dW_temp)\n",
    "    db_tf.append(db_temp)\n",
    "\n",
    "    # For all other layers\n",
    "    for i in reversed(range(1, L)):\n",
    "        dA_temp = tf.matmul(tf.transpose(W_tf[i+1]),dZ_tf[-1])\t\n",
    "        dZ_temp = tf.identity(dA_temp)                     # Backward pass for ReLU, step 1: copy array\n",
    "        dZ_temp = tf.where(Z_tf[i] > 0, dZ_temp, [0])\n",
    "        dW_temp = tf.matmul(dZ_temp, tf.transpose(A_tf[i-1])) / m\n",
    "        db_temp = tf.math.reduce_sum(dZ_temp, axis=1, keepdims=True)/m\n",
    "        dA_tf.append(dA_temp)\n",
    "        dZ_tf.append(dZ_temp)\n",
    "        dW_tf.append(dW_temp)\n",
    "        db_tf.append(db_temp)\n",
    "\n",
    "    # Reverse the list so that indexes are respective to layer number.\n",
    "    # This is so as for backward pass, all calculations are done from last layer to first layer\n",
    "    dA_tf.append(0)    # Append addition 0 so that after reverse, dA0 correspond to A0 with same dimension\n",
    "    dZ_tf.append(0)\n",
    "    dW_tf.append(0)\n",
    "    db_tf.append(0)\n",
    "    dA_tf = list(reversed(dA_tf))\n",
    "    dZ_tf = list(reversed(dZ_tf))\n",
    "    dW_tf = list(reversed(dW_tf))\n",
    "    db_tf = list(reversed(db_tf))\n",
    "\n",
    "    print(dW_tf[2], '\\n', dW_torch[2], '\\n', dW_numpy[2])\n",
    "    print(A_tf[0], '\\n', 'A_torch[0]', '\\n', A_numpy[0])\n",
    "    \n",
    "    return dA_numpy, dZ_numpy, dW_numpy, db_numpy, \\\n",
    "            dA_torch, dZ_torch, dW_torch, db_torch, \\\n",
    "            dA_tf, dZ_tf, dW_tf, db_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(dW_numpy, db_numpy, dW_torch, db_torch, dW_tf, db_tf, W_numpy, b_numpy, W_torch, b_torch, W_tf, b_tf):\n",
    "    # Update parameters\n",
    "    learn_rate = 0.05\n",
    "\n",
    "    # Numpy update\n",
    "    for i in range(1,L+1):\n",
    "        W_numpy[i] = W_numpy[i] - learn_rate*dW_numpy[i]\n",
    "        b_numpy[i] = b_numpy[i] - learn_rate*db_numpy[i]\n",
    "\n",
    "    # Pytorch update\n",
    "    for i in range(1,L+1):\n",
    "        W_torch[i] = W_torch[i] - learn_rate*dW_torch[i]\n",
    "        b_torch[i] = b_torch[i] - learn_rate*db_torch[i]\n",
    "\n",
    "    # Tensorflow update\n",
    "    for i in range(1,L+1):\n",
    "        W_tf[i] = W_tf[i] - learn_rate*dW_tf[i]\n",
    "        b_tf[i] = b_tf[i] - learn_rate*db_tf[i]\n",
    "        \n",
    "    return W_numpy, b_numpy, W_torch, b_torch, W_tf, b_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_accuracy(A_numpy, A_torch, A_tf, Y_numpy, Y_torch, Y_tf):\n",
    "    # Predict output using the output of the last activation\n",
    "\n",
    "    # Numpy prediction\n",
    "    predict_numpy = A_numpy[L].round()\n",
    "    accuracy_numpy = np.count_nonzero(predict_numpy == Y_numpy) / Y_numpy.shape[1] * 100\n",
    "\n",
    "    # Pytorch prediction\n",
    "    predict_torch = A_torch[L].round()\n",
    "    accuracy_torch = torch.sum(predict_torch == Y_torch) / float(Y_torch.shape[1]) * 100\n",
    "\n",
    "    # Tensorflow prediction\n",
    "    predict_tf = tf.math.round(A_tf[L])\n",
    "    accuracy_tf = tf.reduce_sum(tf.cast(predict_tf == Y_tf, tf.float32)) / Y_tf.shape[1] * 100\n",
    "\n",
    "    print('Accuracy of numpy, pytorch and tensorflow prediction are {}%, {}% and {}%respectively\\n'.format(accuracy_numpy, accuracy_torch, accuracy_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 200\n",
    "for i in range(1, EPOCH+1):\n",
    "    print('Epoch {}'.format(i))\n",
    "    Z_numpy, A_numpy, Z_torch, A_torch, Z_tf, A_tf = forward_pass(X, W_numpy, b_numpy, W_torch, b_torch, W_tf, b_tf, Y, n_l, L, m)\n",
    "    dA_numpy, dZ_numpy, dW_numpy, db_numpy, dA_torch, dZ_torch, dW_torch, db_torch, dA_tf, dZ_tf, dW_tf, db_tf = backward_pass(Z_numpy, A_numpy, Z_torch, A_torch, Z_tf, A_tf, Y_numpy, Y_torch, Y_tf, L, m)\n",
    "    W_numpy, b_numpy, W_torch, b_torch, W_tf, b_tf = update_params(dW_numpy, db_numpy, dW_torch, db_torch, dW_tf, db_tf, W_numpy, b_numpy, W_torch, b_torch, W_tf, b_tf)\n",
    "    predict_accuracy(A_numpy, A_torch, A_tf, Y_numpy, Y_torch, Y_tf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
